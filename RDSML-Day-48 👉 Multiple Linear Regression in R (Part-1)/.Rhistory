var1= 500
print('var1')
print(var1)
var_2= mobile
var_2= 'mobile'
print(var_2)
assign('var5', 400)
print(var5)
if_=30
if2= 'mobile phone'
print(class(if_))
print(class(if2))
print(ls())
#find variables
print(ls(all.names = TRUE))
rm(var5)
print(ls(all.names = TRUE))
var5=300
print(ls(all.names = TRUE))
6*8
library(readxl)
data_MoST_24_25 <- read_excel("Desktop/Article/R/All works/data MoST 24-25.xlsx",
sheet = "Sheet4", range = "A1:AA87")
View(data_MoST_24_25)
library(readxl)
pc <- read_excel("Desktop/Article/R/All works/data MoST 24-25.xlsx",
sheet = "Sheet4", range = "A1:AA87")
View(pc)
library(devtools)
install.packages('devtools')
library(devtools)
library(devtools)
library(ggbiplot)
library(devtools)
library(ggbiplot)
library(ggplot2)
pca = prcomp(pc[-1],
center = TRUE,
scale. = TRUE)
pca$scale
print(pca)
summary(pca)
g = ggbiplot(pca,
obs.scale = 1,
var.scale = 1,
groups = pc$water,
ellipse = TRUE,
circle = TRUE,
ellipse.prob = 0.68)
View(g)
g = ggbiplot(pca,
obs.scale = 1,
var.scale = 1,
groups = pc$water,
ellipse = TRUE,
circle = TRUE,
ellipse.prob = 0.68)
g = ggbiplot(pca,
obs.scale = 1,
var.scale = 1,
groups = pc$water,
ellipse = TRUE,
circle = TRUE,
ellipse.prob = 0.68)+
scale_color_discrete(name = '') +
theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
g = ggbiplot(pca,
obs.scale = 1,
var.scale = 1,
groups = pc$water,
ellipse = TRUE,
circle = TRUE,
ellipse.prob = 0.68)+
scale_color_discrete(name = '') +
theme(legend.direction = 'horizontal',
legend.position = 'right')
g = ggbiplot(pca,
obs.scale = 1,
var.scale = 1,
groups = pc$water,
ellipse = TRUE,
circle = TRUE,
ellipse.prob = 0.68)+
scale_color_discrete(name = '') +
theme(legend.direction = 'horizontal',
legend.position = 'right')
print(g)
g = ggbiplot(pca,
obs.scale = 1,
var.scale = 1,
groups = pc$water,
ellipse = TRUE,
circle = TRUE,
ellipse.prob = 0.68)+
scale_color_discrete(name = '') +
theme(legend.direction = 'vertical',
legend.position = 'top')
print(g)
rnorm(3, mean = 0.92, sd = 0.05)
rnorm(3, mean = 1.24, sd = 0.03)
rnorm(3, mean = 75.93, sd = 0.34)
rnorm(3, mean = 1.66, sd = 0.02)
rnorm(3, mean = 1.45, sd = 0.07)
rnorm(3, mean = 1.26, sd = 0.04)
rnorm(3, mean = 81.65, sd = 0.12)
rnorm(3, mean = 1.41, sd = 0.04)
rnorm(3, mean = 3.47, sd = 0.24)
rnorm(3, mean = 1.15, sd = 0.003)
rnorm(3, mean = 85.10, sd = 0.21)
rnorm(3, mean = 1.13, sd = 0.009)
setwd("~/Desktop/Article/R/All works/R-for-Data-Science-and-Machine-Learning-Batch8-NBICT/RDSML-Day-48 ðŸ‘‰ Multiple Linear Regression in R (Part-1)")
#Multiple linear regression
#Importing the data set
dataset = read.csv('50_Startups.csv')
#What is multiple linear regression?
#Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between one dependent variable and two or more independent variables.
#Through multiple linear regression, we can analyze how multiple factors simultaneously influence a single outcome.
#This technique helps us understand the combined effect of several predictors on the response variable.
#It is commonly used in various fields such as economics, finance, social sciences, and healthcare to make predictions and infer relationships between variables.
#In multiple linear regression, statistically significant independent variables are identified, and their coefficients indicate the strength and direction of their impact on the dependent variable.
#Encoding categorical data
dataset$State = factor(dataset$State, levels = c('New York', 'California', 'Florida'), labels = c(1, 2, 3))
#Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
#Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State, data = training_set)
summary(regressor)
#Multiple r squared:  0.9507,	Adjusted R-squared:  0.9348 means that 93.48% of the variance in the dependent variable (Profit) can be explained by the independent variables (R.D.Spend, Administration, Marketing.Spend, and State) in the model.
summary(regressor)
#Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
print(y_pred)
regressor = lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend, data = training_set)
summary(regressor)
regressor = lm(formula = Profit ~ R.D.Spend+ Marketing.Spend + State, data = training_set)
summary(regressor)
regressor = lm(formula = Profit ~ R.D.Spend+ Marketing.Spend, data = training_set)
summary(regressor)
regressor = lm(formula = Profit ~ R.D.Spend, data = training_set)
summary(regressor)
#Automated stepwise Backward Elimination
full_model = lm(Profit ~ ., data = training_set)
summary(full_model)
final_model = step(full_model, direction = "backward")
summary(final_model)
#Assumption checking
par(mfrow=c(2,2))
plot(final_model)
par(mfrow=c(2,2))
plot(final_model)
plot(final_model)
#The residuals vs Fitted plot shows that the residuals are randomly scattered around the horizontal line at 0, indicating that the assumption of linearity is satisfied.
#The Normal Q-Q plot shows that the residuals follow a straight line, indicating that the assumption of normality is satisfied.
#The Scale-Location plot shows that the residuals are evenly spread across the range of fitted values, indicating that the assumption of homoscedasticity is satisfied.
#The Residuals vs Leverage plot helps identify influential data points. No points fall outside the Cook's distance lines, indicating no influential points.
par(mfrow=c(1,1))
#Final prediction
y_pred_final = predict(final_model, newdata = test_set)
print(y_pred_final)
#Assumption checking
cor(training_set$R.D.Spend, training_set$Marketing.Spend)
#Assumption checking
cor(training_set$R.D.Spend, training_set$Marketing.Spend, method = "pearson")
cor.test(training_set$R.D.Spend, training_set$Marketing.Spend, method = "pearson")
#Assumption checking
cor(training_set$R.D.Spend, training_set$Profit, method = "pearson")
cor.test(training_set$Marketing.Spend, training_set$Profit, method = "pearson")
cor.test(training_set$R.D.Spend, training_set$Profit, method = "pearson")
plot(training_set$R.D.Spend, training_set$Profit)
plot(training_set$Marketing.Spend, training_set$Profit)
plot(final_model)
par(mfrow=c(2,2))
plot(final_model)
# Checking for the Independence of errors assumption
library(lmtest)
# Checking for the Independence of errors assumption
install.packages("lmtest")
library(lmtest)
library(lmtest)
dwtest(final_model)
shapiro.test(final_model$residuals)
#Checking the normality of residuals assumption
shapiro.test(rstandard(final_model))
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
install.packages("car")
library(car)
library(car)
vif(final_model)
summary(final_model)
#Checking the homoscedasticity assumption
install.packages("lmtest")
library(lmtest)
bptest(final_model)
plot(final_model$fitted.values,rstandard(final_model))
abline(h=0,col="red")
plot(final_model$fitted.values,rstandard(final_model))
abline(h=0,col="red")
